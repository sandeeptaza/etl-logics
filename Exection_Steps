
Execution
To execute the AWS Glue job, follow these steps:

1. Upload the Script to S3
Save the script (glue_job_etl.py) locally.
Upload the script to an S3 bucket that is accessible by AWS Glue.
2. Create the AWS Glue Job
Navigate to the AWS Glue console.
Click on "Jobs" and then select "Add Job".
Configure the job properties:
Name: Provide a unique name for your job.
IAM Role: Select an IAM role with the necessary permissions to access the Glue Data Catalog and S3.
Script path: Provide the S3 path where the uploaded script is located.
Python version: Choose Python 3.6 or higher.
Worker type and number: Set the number of workers and specify the worker type.
3. Run the Job
Click "Save" and then "Run" to start the job.
Monitor the job execution in the AWS Glue console under "Jobs > Runs".
Error Handling and Logging
The script uses try-except blocks to handle errors at various stages of data processing.
Errors are logged using the Python logging module, which provides detailed information about the error and the operation affected.
Logs can be accessed through Amazon CloudWatch Logs for monitoring and troubleshooting purposes.
Data Quality Checks
Before processing, the script checks for non-empty DataFrames and validates that essential columns are present.
Data quality checks are performed after each major transformation to ensure data integrity.
If any data quality check fails, an error is logged, and the processing for that provider is halted.
Extending the Script
To add support for new providers or modify the processing logic:

Update Configuration:

Add a new entry in the provider_configurations dictionary with the required parameters.
Custom Logic:

If the new provider requires unique processing logic, add the necessary logic within the corresponding functions (e.g., prepare_payin_session_dataframes).
Run the Job:

Execute the AWS Glue job, which will automatically process the new provider based on the updated configuration.